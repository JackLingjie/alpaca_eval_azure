{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v-lingjiang/miniconda3/envs/eval/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-13 13:29:06,453\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 13:29:09 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/Phi-3-small-8k-instruct', speculative_config=None, tokenizer='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/Phi-3-small-8k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/Phi-3-small-8k-instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 11-13 13:29:09 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 11-13 13:29:09 model_runner.py:720] Starting to load model /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/Phi-3-small-8k-instruct...\n",
      "INFO 11-13 13:29:09 selector.py:41] Using BlocksparseFlashAttention backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:06<00:18,  6.10s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:12<00:12,  6.13s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:18<00:06,  6.34s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:19<00:00,  4.08s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:19<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 13:29:29 model_runner.py:732] Loading model weights took 14.5384 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 13:29:30 gpu_executor.py:102] # GPU blocks: 28181, # CPU blocks: 2048\n",
      "INFO 11-13 13:29:33 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-13 13:29:33 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-13 13:29:43 model_runner.py:1225] Graph capturing finished in 10 secs.\n",
      "model name: Phi-3-small-8k-instruct\n",
      "model_path: /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/Phi-3-small-8k-instruct/Phi-3-small-8k-instruct\n",
      "use original template\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'messages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     gen_kwargs_vllm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop_token_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39meos_token_id, tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|end_of_text|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse original template\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m messages \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\u001b[43mmessages\u001b[49m, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs_vllm)\n\u001b[1;32m     47\u001b[0m eval_set \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtatsu-lab/alpaca_eval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpaca_eval\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'messages' is not defined"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "import json\n",
    "import argparse \n",
    "\n",
    "parser = argparse.ArgumentParser(description='Set model name.')  \n",
    "# parser.add_argument('--model-name', type=str, required=True, help='Name of the model to use')  \n",
    "# parser.add_argument('--model-path', type=str, default=\"/home/lidong1/jianglingjie/LLama-Factory/model_checkpoint/huggingface\", help='Dir of the model to use') \n",
    "# args = parser.parse_args()  \n",
    "# path_dir = args.model_path\n",
    "# model_name = args.model_name \n",
    "\n",
    "# template = \"{{ '<|begin_of_text|>' }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\"\n",
    "# template = \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message + '\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ 'Human: ' + content + '\\nAssistant:' }}{% elif message['role'] == 'assistant' %}{{ content + '<|end_of_text|>' + '\\n' }}{% endif %}{% endfor %}\"\n",
    "model_name = \"Phi-3-small-8k-instruct\"\n",
    "path_dir = \"/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/Phi-3-small-8k-instruct\"\n",
    "# path_dir = \"Phi-3-small-8k-instruct\"\n",
    "# model_name = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"Meta-Llama-3.1-8B\"\n",
    "# Create an LLM.\n",
    "llm = LLM(model=f\"{path_dir}\", trust_remote_code=True)\n",
    "\n",
    "print(f\"model name: {model_name}\")\n",
    "print(f\"model_path: {path_dir}/{model_name}\")\n",
    "\n",
    "gen_kwargs_vllm = {\n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "tokenizer = llm.get_tokenizer()\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = template\n",
    "    # tokenizer.chat_template = tokenizer.chat_template.replace(\"<|eot_id|>\", tokenizer.eos_token)\n",
    "    # tokenizer.chat_template\n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "    print(f\"tokenizer.chat_template: {tokenizer.chat_template}\")\n",
    "    print(\"tokenizer is None, use setted template\")\n",
    "else:\n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]\n",
    "    print(\"use original template\")\n",
    "messages = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(**gen_kwargs_vllm)\n",
    "\n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]\n",
    "\n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "eval_set = eval_set.map(convert_to_message)\n",
    "\n",
    "encoded_inputs = tokenizer.batch_encode_plus(  \n",
    "    eval_set['messages'],  \n",
    "    add_special_tokens=False,\n",
    ") \n",
    "input_ids = encoded_inputs['input_ids']  \n",
    "\n",
    "outputs = llm.generate(prompt_token_ids=input_ids, sampling_params=sampling_params)\n",
    "outputs_text = [x.outputs[0].text for x in outputs]\n",
    "eval_set = eval_set.remove_columns([\"output\"])  # Remove the existing 'output' column if it exists  \n",
    "eval_set = eval_set.remove_columns([\"messages\"])\n",
    "eval_set = eval_set.add_column(\"output\", outputs_text)  \n",
    "def rename_generator(sample):\n",
    "    sample['generator'] = f\"{model_name}\"\n",
    "    return sample\n",
    "eval_set = eval_set.map(rename_generator)\n",
    "eval_set.to_json(f\"{model_name}.jsonl\", batch_size=128, num_proc=8)\n",
    "\n",
    "## save data\n",
    "\n",
    "export_dataset = eval_set\n",
    "export_data_list = [dict(row) for row in export_dataset]\n",
    "print(f\"export data length {len(export_data_list)}\")\n",
    "# with open(f'./data/{model_name}.json', 'w', encoding='utf-8') as f:  \n",
    "#     json.dump(export_data_list, f, ensure_ascii=False, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers  \n",
    "import torch  \n",
    "from tqdm import tqdm  \n",
    "import datasets  \n",
    "import json  \n",
    "import argparse  \n",
    "  \n",
    "# Argument parsing  \n",
    "parser = argparse.ArgumentParser(description='Set model name.')    \n",
    "parser.add_argument('--model-name', type=str, required=True, help='Name of the model to use')    \n",
    "parser.add_argument('--model-path', type=str, default=\"/home/lidong1/jianglingjie/LLama-Factory/model_checkpoint/huggingface\", help='Dir of the model to use')   \n",
    "args = parser.parse_args()    \n",
    "  \n",
    "path_dir = args.model_path  \n",
    "model_name = args.model_name   \n",
    "  \n",
    "# Load the tokenizer and model  \n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(path_dir, trust_remote_code=True)  \n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(path_dir, trust_remote_code=True, torch_dtype=torch.bfloat16)  \n",
    "model.to(\"cuda\")  \n",
    "  \n",
    "print(f\"model name: {model_name}\")  \n",
    "print(f\"model_path: {path_dir}/{model_name}\")  \n",
    "  \n",
    "gen_kwargs_hf = {  \n",
    "    \"max_new_tokens\": 2048,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"do_sample\": False,  \n",
    "}  \n",
    "  \n",
    "def generate_response(prompt):  \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)  \n",
    "    tokens = model.generate(**inputs, **gen_kwargs_hf)  \n",
    "    prompt = prompt.replace(\"<|begin_of_text|>\", \"\")\n",
    "    return tokenizer.decode(tokens[0], skip_special_tokens=True).replace(prompt, '')  \n",
    "  \n",
    "# Load dataset  \n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]  \n",
    "  \n",
    "# Convert dataset examples to messages  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.convert_tokens_to_string(tokenizer.tokenize(tokenizer.chat_template.format(messages=messages)))  \n",
    "    return example    \n",
    "  \n",
    "eval_set = eval_set.map(convert_to_message)  \n",
    "  \n",
    "# Generate responses  \n",
    "outputs_text = []  \n",
    "for example in tqdm(eval_set):  \n",
    "    response = generate_response(example['messages'])  \n",
    "    outputs_text.append(response)  \n",
    "  \n",
    "# Update dataset with model outputs  \n",
    "eval_set = eval_set.remove_columns([\"output\"])  # Remove the existing 'output' column if it exists    \n",
    "eval_set = eval_set.remove_columns([\"messages\"])  \n",
    "eval_set = eval_set.add_column(\"output\", outputs_text)  \n",
    "  \n",
    "def rename_generator(sample):  \n",
    "    sample['generator'] = f\"{model_name}\"  \n",
    "    return sample  \n",
    "  \n",
    "eval_set = eval_set.map(rename_generator)  \n",
    "eval_set.to_json(f\"{model_name}.jsonl\", batch_size=128, num_proc=8)  \n",
    "  \n",
    "# Save data  \n",
    "export_dataset = eval_set  \n",
    "export_data_list = [dict(row) for row in export_dataset]  \n",
    "print(f\"export data length {len(export_data_list)}\")  \n",
    "with open(f'./data/{model_name}.json', 'w', encoding='utf-8') as f:  \n",
    "    json.dump(export_data_list, f, ensure_ascii=False, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n",
      "Map:   0%|          | 0/805 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'% if messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_string(tokenizer\u001b[38;5;241m.\u001b[39mtokenize(tokenizer\u001b[38;5;241m.\u001b[39mchat_template\u001b[38;5;241m.\u001b[39mformat(messages\u001b[38;5;241m=\u001b[39mmessages)))  \n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example    \n\u001b[0;32m---> 31\u001b[0m eval_set \u001b[38;5;241m=\u001b[39m \u001b[43meval_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_to_message\u001b[49m\u001b[43m)\u001b[49m  \n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/datasets/arrow_dataset.py:3035\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3031\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3032\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3033\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3034\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3035\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3036\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3037\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/datasets/arrow_dataset.py:3408\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3406\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3408\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3410\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/datasets/arrow_dataset.py:3300\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3299\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3300\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3302\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3303\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3304\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36mconvert_to_message\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_message\u001b[39m(example):  \n\u001b[1;32m     27\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m]}]  \n\u001b[0;32m---> 28\u001b[0m     example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_string(tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m))  \n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example\n",
      "\u001b[0;31mKeyError\u001b[0m: '% if messages'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import datasets  \n",
    "import json  \n",
    "device = \"cuda\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"/mnt/lingjiejiang/textual_aesthetics/exp/saves/bitnet_glan2_2048_default_template/fullft_lr2e5_4kstep/sft/checkpoint-3000\", trust_remote_code=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"/mnt/lingjiejiang/textual_aesthetics/exp/saves/bitnet_glan2_2048_default_template/fullft_lr2e5_4kstep/sft/checkpoint-3000\", trust_remote_code=True, torch_dtype=torch.bfloat16, attn_implementation = \"flash_attention_2\")\n",
    "model.to(device)\n",
    "gen_kwargs_hf = {  \n",
    "    \"max_new_tokens\": 2048,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"do_sample\": False,  \n",
    "}  \n",
    "  \n",
    "def generate_response(prompt):  \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)  \n",
    "    tokens = model.generate(**inputs, **gen_kwargs_hf)  \n",
    "    return tokenizer.decode(tokens[0], skip_special_tokens=True).replace(prompt, '')  \n",
    "  \n",
    "# Load dataset  \n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]  \n",
    "  \n",
    "# Convert dataset examples to messages  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "   \n",
    "  \n",
    "eval_set = eval_set.map(convert_to_message)  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,  35075,     25,   3639,    527,    279,   5144,    315,   1063,\n",
      "          11495,  20142,    430,   3940,    872,  31133,    389,  37776,   5380,\n",
      "          72803,     25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "<|begin_of_text|>Human: What are the names of some famous actors that started their careers on Broadway?\n",
      "Assistant:\n",
      "{'input_ids': tensor([[128000,  35075,     25,   2650,   1550,   2326,   5415,    636,    872,\n",
      "           5144,   5380,  72803,     25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "<|begin_of_text|>Human: How did US states get their names?\n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "gen_kwargs_hf = {  \n",
    "    \"max_new_tokens\": 2048,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"do_sample\": False,  \n",
    "}  \n",
    "  \n",
    "def generate_response(prompt):  \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)  \n",
    "    print(inputs)\n",
    "    # print(tokenizer.decode(inputs))\n",
    "    # 从inputs中获取input_ids  \n",
    "    input_ids = inputs[\"input_ids\"]  \n",
    "    \n",
    "    # 将input_ids解码回字符串  \n",
    "    decoded_string = tokenizer.decode(input_ids[0])  \n",
    "    \n",
    "    print(decoded_string)\n",
    "    return \"\"\n",
    "    # tokens = model.generate(**inputs, **gen_kwargs_hf)  \n",
    "    # return tokenizer.decode(tokens[0], skip_special_tokens=True).replace(prompt, '')  \n",
    "  \n",
    "# Load dataset  \n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]  \n",
    "  \n",
    "# Convert dataset examples to messages  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "  \n",
    "eval_set = eval_set.map(convert_to_message)  \n",
    "  \n",
    "# Generate responses  \n",
    "outputs_text = []  \n",
    "for example in eval_set.select(range(2)):  \n",
    "    response = generate_response(example['messages'])  \n",
    "    outputs_text.append(response)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 805/805 [00:00<00:00, 14207.81 examples/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/v-lingjiang/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,  35075,     25,   3639,    527,    279,   5144,    315,   1063,\n",
      "          11495,  20142,    430,   3940,    872,  31133,    389,  37776,   5380,\n",
      "          72803,     25, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001],\n",
      "        [128000,  35075,     25,   2650,   1550,   2326,   5415,    636,    872,\n",
      "           5144,   5380,  72803,     25, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001],\n",
      "        [128000,  35075,     25,  21694,     11,    856,  13219,    323,   1077,\n",
      "          85612,   1390,    757,    311,   1514,  10536,   4047,    449,   1124,\n",
      "             13,   3053,    499,  10552,   1268,    279,   1847,    374,   6476,\n",
      "             11,    779,    814,   1541,    956,   1935,   9610,    315,    757,\n",
      "           5380,  72803,     25],\n",
      "        [128000,  35075,     25,   3639,    374,   1063,   7155,   4731,    505,\n",
      "            279,    220,   5926,     15,     82,   5380,  72803,     25, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001],\n",
      "        [128000,  35075,     25,   2650,    656,    358,  15411,    264,   3118,\n",
      "          63266,   5380,  72803,     25, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:21<00:00, 201.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# print(f\"model name: {model_name}\")  \n",
    "# print(f\"model_path: {path_dir}/{model_name}\")  \n",
    "  \n",
    "from tqdm import tqdm\n",
    "gen_kwargs_hf = {  \n",
    "    \"max_new_tokens\": 2048,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"do_sample\": False,  \n",
    "}  \n",
    "  \n",
    "def generate_responses(prompts):  \n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, add_special_tokens=False).to(model.device)  \n",
    "    print(inputs)\n",
    "    tokens = model.generate(**inputs, **gen_kwargs_hf)  \n",
    "    return [tokenizer.decode(t, skip_special_tokens=True).replace(prompt, '') for t, prompt in zip(tokens, prompts)]  \n",
    "  \n",
    "# Load dataset  \n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]  \n",
    "  \n",
    "# Convert dataset examples to messages  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "  \n",
    "eval_set = eval_set.map(convert_to_message)  \n",
    "eval_set = eval_set.select(range(5))\n",
    "# Generate responses in batches  \n",
    "batch_size = 32  \n",
    "outputs_text = []  \n",
    "for i in tqdm(range(0, len(eval_set), batch_size)):  \n",
    "    batch_messages = eval_set[i:i+batch_size]['messages']  \n",
    "    responses = generate_responses(batch_messages)  \n",
    "    outputs_text.extend(responses)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Human: hello\\nAssistant:'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers  \n",
    "import torch  \n",
    "from tqdm import tqdm  \n",
    "import datasets  \n",
    "import json  \n",
    "import argparse  \n",
    "  \n",
    "# Argument parsing  \n",
    "parser = argparse.ArgumentParser(description='Set model name.')  \n",
    "parser.add_argument('--model-name', type=str, required=True, help='Name of the model to use')  \n",
    "parser.add_argument('--model-path', type=str, default=\"/home/lidong1/jianglingjie/LLama-Factory/model_checkpoint/huggingface\", help='Dir of the model to use')  \n",
    "args = parser.parse_args()  \n",
    "  \n",
    "path_dir = args.model_path  \n",
    "model_name = args.model_name  \n",
    "  \n",
    "# Load the tokenizer and model  \n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(path_dir, trust_remote_code=True)  \n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(path_dir, trust_remote_code=True, torch_dtype=torch.bfloat16)  \n",
    "model.to(\"cuda\")  \n",
    "  \n",
    "print(f\"model name: {model_name}\")  \n",
    "print(f\"model_path: {path_dir}/{model_name}\")  \n",
    "  \n",
    "gen_kwargs_hf = {  \n",
    "    \"max_new_tokens\": 2048,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"do_sample\": False,  \n",
    "}  \n",
    "  \n",
    "gen_kwargs_hf['eos_token_id'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]\n",
    "gen_kwargs_hf['pad_token_id'] = tokenizer.eos_token_id \n",
    "\n",
    "def generate_response(prompt):  \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)  \n",
    "    tokens = model.generate(**inputs, **gen_kwargs_hf)  \n",
    "    prompt = prompt.replace(\"<|begin_of_text|>\", \"\")\n",
    "    return tokenizer.decode(tokens[0], skip_special_tokens=True).replace(prompt, '')  \n",
    "  \n",
    "# Load dataset  \n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]  \n",
    "  \n",
    "# Convert dataset examples to messages  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "  \n",
    "eval_set = eval_set.map(convert_to_message)  \n",
    "  \n",
    "# Generate responses  \n",
    "outputs_text = []  \n",
    "for example in tqdm(eval_set):  \n",
    "    response = generate_response(example['messages'])  \n",
    "    outputs_text.append(response)  \n",
    "  \n",
    "# Update dataset with model outputs  \n",
    "eval_set = eval_set.remove_columns([\"output\"])  # Remove the existing 'output' column if it exists  \n",
    "eval_set = eval_set.remove_columns([\"messages\"])  \n",
    "eval_set = eval_set.add_column(\"output\", outputs_text)  \n",
    "  \n",
    "def rename_generator(sample):  \n",
    "    sample['generator'] = f\"{model_name}\"  \n",
    "    return sample  \n",
    "  \n",
    "eval_set = eval_set.map(rename_generator)  \n",
    "eval_set.to_json(f\"{model_name}.jsonl\", batch_size=128, num_proc=8)  \n",
    "  \n",
    "# Save data  \n",
    "export_dataset = eval_set  \n",
    "export_data_list = [dict(row) for row in export_dataset]  \n",
    "print(f\"export data length {len(export_data_list)}\")  \n",
    "with open(f'./data/{model_name}.json', 'w', encoding='utf-8') as f:  \n",
    "    json.dump(export_data_list, f, ensure_ascii=False, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 805/805 [00:00<00:00, 14363.26 examples/s]\n",
      "  0%|          | 0/805 [00:00<?, ?it/s]/home/v-lingjiang/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "  0%|          | 3/805 [03:25<15:40:41, 70.38s/it]"
     ]
    }
   ],
   "source": [
    "  \n",
    "gen_kwargs_hf = {  \n",
    "    \"max_new_tokens\": 2048,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"do_sample\": False,  \n",
    "}  \n",
    "  \n",
    "gen_kwargs_hf['eos_token_id'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]\n",
    "gen_kwargs_hf['pad_token_id'] = tokenizer.eos_token_id \n",
    "\n",
    "def generate_response(prompt):  \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)  \n",
    "    tokens = model.generate(**inputs, **gen_kwargs_hf)  \n",
    "    input = prompt.replace(\"<|begin_of_text|>\", \"\")\n",
    "    return tokenizer.decode(tokens[0], skip_special_tokens=True).replace(prompt, '')  \n",
    "  \n",
    "# Load dataset  \n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]  \n",
    "  \n",
    "# Convert dataset examples to messages  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "  \n",
    "eval_set = eval_set.map(convert_to_message)  \n",
    "  \n",
    "# Generate responses  \n",
    "outputs_text = []  \n",
    "for example in tqdm(eval_set):  \n",
    "    response = generate_response(example['messages'])  \n",
    "    outputs_text.append(response)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v-lingjiang/miniconda3/envs/eval/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import datasets  \n",
    "import json  \n",
    "from tqdm import tqdm\n",
    "device = \"cuda\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"/mnt/lingjiejiang/textual_aesthetics/exp/saves/bitnet_glanchat_v2.1_8b_2048_default_template/fullft_lr5e6_e3/sft/checkpoint-3500\", trust_remote_code=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"/mnt/lingjiejiang/textual_aesthetics/exp/saves/bitnet_glanchat_v2.1_8b_2048_default_template/fullft_lr5e6_e3/sft/checkpoint-3500\", trust_remote_code=True, torch_dtype=torch.bfloat16, attn_implementation = \"flash_attention_2\")\n",
    "model.to(device)\n",
    "\n",
    "  \n",
    "# gen_kwargs_hf = {  \n",
    "#     \"max_new_tokens\": 2048,  \n",
    "#     \"temperature\": 0.7,\n",
    "#     \"top_p\":0.95,  \n",
    "#     \"do_sample\": True,  \n",
    "# }  \n",
    "  \n",
    "# gen_kwargs_hf['eos_token_id'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]\n",
    "# gen_kwargs_hf['pad_token_id'] = tokenizer.eos_token_id \n",
    "\n",
    "# def generate_response(prompt):  \n",
    "#     print(f\"prompt: {prompt}\")\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)  \n",
    "#     print(f\"inputs:{inputs}\")\n",
    "#     tokens = model.generate(**inputs, **gen_kwargs_hf)  \n",
    "#     res = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "#     print(res)\n",
    "#     return tokenizer.decode(tokens[0], skip_special_tokens=True).replace(prompt, '')  \n",
    "  \n",
    "# # Load dataset  \n",
    "# eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]  \n",
    "  \n",
    "# # Convert dataset examples to messages  \n",
    "# def convert_to_message(example):  \n",
    "#     messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "#     example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "#     return example  \n",
    "  \n",
    "# eval_set = eval_set.map(convert_to_message)  \n",
    "  \n",
    "# # Generate responses  \n",
    "# outputs_text = []  \n",
    "# for example in tqdm(eval_set):  \n",
    "#     response = generate_response(example['messages'])  \n",
    "#     print(response)\n",
    "#     outputs_text.append(response)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/805 [00:00<?, ?it/s]/home/v-lingjiang/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: <|begin_of_text|>Human: What are the names of some famous actors that started their careers on Broadway?\n",
      "Assistant:\n",
      "inputs:{'input_ids': tensor([[128000,  35075,     25,   3639,    527,    279,   5144,    315,   1063,\n",
      "          11495,  20142,    430,   3940,    872,  31133,    389,  37776,   5380,\n",
      "          72803,     25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/805 [00:08<1:52:52,  8.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: Human: What are the names of some famous actors that started their careers on Broadway?\n",
      "Assistant:Some famous actors who started their careers on Broadway include:\n",
      "\n",
      "1. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "2. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "3. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "4. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "5. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "\n",
      "These actors are just a few examples, but there are many more who have made significant contributions to the world of theater.\n",
      "res_raw: <|begin_of_text|>Human: What are the names of some famous actors that started their careers on Broadway?\n",
      "Assistant:Some famous actors who started their careers on Broadway include:\n",
      "\n",
      "1. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "2. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "3. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "4. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "5. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "\n",
      "These actors are just a few examples, but there are many more who have made significant contributions to the world of theater.<|end_of_text|>\n",
      "Some famous actors who started their careers on Broadway include:\n",
      "\n",
      "1. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "2. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "3. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "4. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "5. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "\n",
      "These actors are just a few examples, but there are many more who have made significant contributions to the world of theater.\n",
      "prompt: <|begin_of_text|>Human: How did US states get their names?\n",
      "Assistant:\n",
      "inputs:{'input_ids': tensor([[128000,  35075,     25,   2650,   1550,   2326,   5415,    636,    872,\n",
      "           5144,   5380,  72803,     25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/805 [02:36<20:13:51, 90.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: Human: How did US states get their names?\n",
      "Assistant:The names of US states are derived from various sources, including Native American tribes, European explorers, and even the names of the original thirteen colonies. Here are a few examples:\n",
      "\n",
      "1. **Native American Tribes**: Many states are named after Native American tribes that lived in the area. For example, California is named after the Chumash tribe, and Arizona is named after the Apache tribe.\n",
      "\n",
      "2. **European Explorers**: Some states were named after explorers who discovered the area. For example, Florida was named after the Spanish explorer Juan Ponce de León, and Texas was named after the French explorer René de Laudre.\n",
      "\n",
      "3. **Original Thirteen Colonies**: Some states were named after the original thirteen colonies that were established in the 17th and 18th centuries. For example, Virginia was named after the English explorer John Smith, and New York was named after the Dutch explorer Henry Hudson.\n",
      "\n",
      "4. **State Names**: Some states were named after famous figures or events. For example, California was named after the gold rush, and Texas was named after the Texas Revolution.\n",
      "\n",
      "5. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "6. **State Names**: Some states were named after the state capitals of other states. For example, California was named after the state capital of California, and Texas was named after the state capital of Texas.\n",
      "\n",
      "7. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "8. **State Names**: Some states were named after the state capitals of other countries. For example, California was named after the state capital of California, and Texas was named after the state capital of Texas.\n",
      "\n",
      "9. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "10. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "11. **State Names**: Some states were named after the state capitals of other countries. For example, California was named after the state capital of California, and Texas was named after the state capital of Texas.\n",
      "\n",
      "12. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "13. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "14. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "15. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "16. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "17. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "18. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "19. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "20. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "21. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "22. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "23. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "24. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "25. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "26. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "27. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "28. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "29. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "30. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "31. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "32. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "33. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "34. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "35. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "36. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "37. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "38. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "39. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "40. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "41. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "42. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "43. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "44. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "45. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "46. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after\n",
      "res_raw: <|begin_of_text|>Human: How did US states get their names?\n",
      "Assistant:The names of US states are derived from various sources, including Native American tribes, European explorers, and even the names of the original thirteen colonies. Here are a few examples:\n",
      "\n",
      "1. **Native American Tribes**: Many states are named after Native American tribes that lived in the area. For example, California is named after the Chumash tribe, and Arizona is named after the Apache tribe.\n",
      "\n",
      "2. **European Explorers**: Some states were named after explorers who discovered the area. For example, Florida was named after the Spanish explorer Juan Ponce de León, and Texas was named after the French explorer René de Laudre.\n",
      "\n",
      "3. **Original Thirteen Colonies**: Some states were named after the original thirteen colonies that were established in the 17th and 18th centuries. For example, Virginia was named after the English explorer John Smith, and New York was named after the Dutch explorer Henry Hudson.\n",
      "\n",
      "4. **State Names**: Some states were named after famous figures or events. For example, California was named after the gold rush, and Texas was named after the Texas Revolution.\n",
      "\n",
      "5. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "6. **State Names**: Some states were named after the state capitals of other states. For example, California was named after the state capital of California, and Texas was named after the state capital of Texas.\n",
      "\n",
      "7. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "8. **State Names**: Some states were named after the state capitals of other countries. For example, California was named after the state capital of California, and Texas was named after the state capital of Texas.\n",
      "\n",
      "9. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "10. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "11. **State Names**: Some states were named after the state capitals of other countries. For example, California was named after the state capital of California, and Texas was named after the state capital of Texas.\n",
      "\n",
      "12. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "13. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "14. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "15. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "16. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "17. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "18. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "19. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "20. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "21. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "22. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "23. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "24. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "25. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "26. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "27. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "28. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "29. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "30. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "31. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "32. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "33. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "34. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "35. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "36. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "37. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "38. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "39. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "40. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "41. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "42. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "43. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "44. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "45. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "46. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after\n",
      "The names of US states are derived from various sources, including Native American tribes, European explorers, and even the names of the original thirteen colonies. Here are a few examples:\n",
      "\n",
      "1. **Native American Tribes**: Many states are named after Native American tribes that lived in the area. For example, California is named after the Chumash tribe, and Arizona is named after the Apache tribe.\n",
      "\n",
      "2. **European Explorers**: Some states were named after explorers who discovered the area. For example, Florida was named after the Spanish explorer Juan Ponce de León, and Texas was named after the French explorer René de Laudre.\n",
      "\n",
      "3. **Original Thirteen Colonies**: Some states were named after the original thirteen colonies that were established in the 17th and 18th centuries. For example, Virginia was named after the English explorer John Smith, and New York was named after the Dutch explorer Henry Hudson.\n",
      "\n",
      "4. **State Names**: Some states were named after famous figures or events. For example, California was named after the gold rush, and Texas was named after the Texas Revolution.\n",
      "\n",
      "5. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "6. **State Names**: Some states were named after the state capitals of other states. For example, California was named after the state capital of California, and Texas was named after the state capital of Texas.\n",
      "\n",
      "7. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "8. **State Names**: Some states were named after the state capitals of other countries. For example, California was named after the state capital of California, and Texas was named after the state capital of Texas.\n",
      "\n",
      "9. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "10. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "11. **State Names**: Some states were named after the state capitals of other countries. For example, California was named after the state capital of California, and Texas was named after the state capital of Texas.\n",
      "\n",
      "12. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "13. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "14. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "15. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "16. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "17. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "18. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "19. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "20. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "21. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "22. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "23. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "24. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "25. **State Names**: Some states were named after the state capitals of other countries. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "26. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "27. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "28. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "29. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "30. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "31. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "32. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "33. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "34. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "35. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "36. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "37. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "38. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "39. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "40. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "41. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "42. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "43. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "44. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "45. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after the state capital of New York, and Texas was named after the state capital of Texas.\n",
      "\n",
      "46. **State Names**: Some states were named after the state capitals of other states. For example, New York was named after\n",
      "prompt: <|begin_of_text|>Human: Hi, my sister and her girlfriends want me to play kickball with them. Can you explain how the game is played, so they don't take advantage of me?\n",
      "Assistant:\n",
      "inputs:{'input_ids': tensor([[128000,  35075,     25,  21694,     11,    856,  13219,    323,   1077,\n",
      "          85612,   1390,    757,    311,   1514,  10536,   4047,    449,   1124,\n",
      "             13,   3053,    499,  10552,   1268,    279,   1847,    374,   6476,\n",
      "             11,    779,    814,   1541,    956,   1935,   9610,    315,    757,\n",
      "           5380,  72803,     25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/805 [03:16<15:04:13, 67.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: Human: Hi, my sister and her girlfriends want me to play kickball with them. Can you explain how the game is played, so they don't take advantage of me?\n",
      "Assistant:Sure, I'd be happy to help! Kickball is a fun game that involves kicking a ball to score points. Here's a step-by-step guide to how the game is played:\n",
      "\n",
      "### 1. Equipment\n",
      "- **Ball**: A soft, round ball is used in the game. It can be made of various materials, but a common choice is a rubber ball.\n",
      "- **Field**: A rectangular field is used as the playing surface. The field can be made of grass, sand, or any other material.\n",
      "- **Goalposts**: Two goalposts are placed at each end of the field. These posts serve as the scoring points.\n",
      "\n",
      "### 2. Rules\n",
      "- **Objective**: The goal of the game is to score as many points as possible within a given time frame.\n",
      "- **Scoring**: Points are scored by kicking the ball into the goalposts. Each point is worth 1 point.\n",
      "- **Time Frame**: The game is played within a specified time frame, usually 10 minutes or less. If the game is longer than 10 minutes, it is considered a \"full game\" and the score is not counted.\n",
      "\n",
      "### 3. Gameplay\n",
      "- **Kickoff**: The game starts with a kickoff, where the ball is thrown or kicked into the field.\n",
      "- **Scoring**: Players try to score as many points as possible within the 10-minute time frame. They can score points by kicking the ball into the goalposts.\n",
      "- **End of Game**: The game ends when the time is up, and the player with the highest score wins.\n",
      "\n",
      "### 4. Strategies\n",
      "- **Positioning**: Players often position themselves near the goalposts to score as many points as possible.\n",
      "- **Communication**: Players may communicate with each other to coordinate their movements and strategies.\n",
      "- **Agility**: Players need to be agile and quick to react to the ball's movement and to score points.\n",
      "\n",
      "### 5. Fair Play\n",
      "- **No Advantage**: The game is played fairly, and there are no advantages or disadvantages for any player.\n",
      "- **Equal Opportunities**: All players have an equal chance to score points, regardless of their size, strength, or skill level.\n",
      "\n",
      "### 6. Safety\n",
      "- **Safety**: Kickball is a low-impact game, so it is generally safe for players of all ages and abilities.\n",
      "- **Equipment**: The ball is soft and padded, and the field is safe to play on.\n",
      "\n",
      "### Conclusion\n",
      "Kickball is a fun and inclusive game that can be played by people of all ages and abilities. It is a great way to spend time with friends and family, and it encourages teamwork, communication, and fair play. If you have any questions or need further information, feel free to ask!\n",
      "res_raw: <|begin_of_text|>Human: Hi, my sister and her girlfriends want me to play kickball with them. Can you explain how the game is played, so they don't take advantage of me?\n",
      "Assistant:Sure, I'd be happy to help! Kickball is a fun game that involves kicking a ball to score points. Here's a step-by-step guide to how the game is played:\n",
      "\n",
      "### 1. Equipment\n",
      "- **Ball**: A soft, round ball is used in the game. It can be made of various materials, but a common choice is a rubber ball.\n",
      "- **Field**: A rectangular field is used as the playing surface. The field can be made of grass, sand, or any other material.\n",
      "- **Goalposts**: Two goalposts are placed at each end of the field. These posts serve as the scoring points.\n",
      "\n",
      "### 2. Rules\n",
      "- **Objective**: The goal of the game is to score as many points as possible within a given time frame.\n",
      "- **Scoring**: Points are scored by kicking the ball into the goalposts. Each point is worth 1 point.\n",
      "- **Time Frame**: The game is played within a specified time frame, usually 10 minutes or less. If the game is longer than 10 minutes, it is considered a \"full game\" and the score is not counted.\n",
      "\n",
      "### 3. Gameplay\n",
      "- **Kickoff**: The game starts with a kickoff, where the ball is thrown or kicked into the field.\n",
      "- **Scoring**: Players try to score as many points as possible within the 10-minute time frame. They can score points by kicking the ball into the goalposts.\n",
      "- **End of Game**: The game ends when the time is up, and the player with the highest score wins.\n",
      "\n",
      "### 4. Strategies\n",
      "- **Positioning**: Players often position themselves near the goalposts to score as many points as possible.\n",
      "- **Communication**: Players may communicate with each other to coordinate their movements and strategies.\n",
      "- **Agility**: Players need to be agile and quick to react to the ball's movement and to score points.\n",
      "\n",
      "### 5. Fair Play\n",
      "- **No Advantage**: The game is played fairly, and there are no advantages or disadvantages for any player.\n",
      "- **Equal Opportunities**: All players have an equal chance to score points, regardless of their size, strength, or skill level.\n",
      "\n",
      "### 6. Safety\n",
      "- **Safety**: Kickball is a low-impact game, so it is generally safe for players of all ages and abilities.\n",
      "- **Equipment**: The ball is soft and padded, and the field is safe to play on.\n",
      "\n",
      "### Conclusion\n",
      "Kickball is a fun and inclusive game that can be played by people of all ages and abilities. It is a great way to spend time with friends and family, and it encourages teamwork, communication, and fair play. If you have any questions or need further information, feel free to ask!<|end_of_text|>\n",
      "Sure, I'd be happy to help! Kickball is a fun game that involves kicking a ball to score points. Here's a step-by-step guide to how the game is played:\n",
      "\n",
      "### 1. Equipment\n",
      "- **Ball**: A soft, round ball is used in the game. It can be made of various materials, but a common choice is a rubber ball.\n",
      "- **Field**: A rectangular field is used as the playing surface. The field can be made of grass, sand, or any other material.\n",
      "- **Goalposts**: Two goalposts are placed at each end of the field. These posts serve as the scoring points.\n",
      "\n",
      "### 2. Rules\n",
      "- **Objective**: The goal of the game is to score as many points as possible within a given time frame.\n",
      "- **Scoring**: Points are scored by kicking the ball into the goalposts. Each point is worth 1 point.\n",
      "- **Time Frame**: The game is played within a specified time frame, usually 10 minutes or less. If the game is longer than 10 minutes, it is considered a \"full game\" and the score is not counted.\n",
      "\n",
      "### 3. Gameplay\n",
      "- **Kickoff**: The game starts with a kickoff, where the ball is thrown or kicked into the field.\n",
      "- **Scoring**: Players try to score as many points as possible within the 10-minute time frame. They can score points by kicking the ball into the goalposts.\n",
      "- **End of Game**: The game ends when the time is up, and the player with the highest score wins.\n",
      "\n",
      "### 4. Strategies\n",
      "- **Positioning**: Players often position themselves near the goalposts to score as many points as possible.\n",
      "- **Communication**: Players may communicate with each other to coordinate their movements and strategies.\n",
      "- **Agility**: Players need to be agile and quick to react to the ball's movement and to score points.\n",
      "\n",
      "### 5. Fair Play\n",
      "- **No Advantage**: The game is played fairly, and there are no advantages or disadvantages for any player.\n",
      "- **Equal Opportunities**: All players have an equal chance to score points, regardless of their size, strength, or skill level.\n",
      "\n",
      "### 6. Safety\n",
      "- **Safety**: Kickball is a low-impact game, so it is generally safe for players of all ages and abilities.\n",
      "- **Equipment**: The ball is soft and padded, and the field is safe to play on.\n",
      "\n",
      "### Conclusion\n",
      "Kickball is a fun and inclusive game that can be played by people of all ages and abilities. It is a great way to spend time with friends and family, and it encourages teamwork, communication, and fair play. If you have any questions or need further information, feel free to ask!\n",
      "prompt: <|begin_of_text|>Human: What is some cool music from the 1920s?\n",
      "Assistant:\n",
      "inputs:{'input_ids': tensor([[128000,  35075,     25,   3639,    374,   1063,   7155,   4731,    505,\n",
      "            279,    220,   5926,     15,     82,   5380,  72803,     25]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "gen_kwargs_hf = {  \n",
    "    \"max_new_tokens\": 2048,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"do_sample\": False,  \n",
    "}  \n",
    "  \n",
    "gen_kwargs_hf['eos_token_id'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]\n",
    "gen_kwargs_hf['pad_token_id'] = tokenizer.eos_token_id \n",
    "\n",
    "def generate_response(prompt):  \n",
    "    print(f\"prompt: {prompt}\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)  \n",
    "    print(f\"inputs:{inputs}\")\n",
    "    tokens = model.generate(**inputs, **gen_kwargs_hf)  \n",
    "    res = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "    print(f\"res: {res}\")\n",
    "    res_raw = tokenizer.decode(tokens[0], skip_special_tokens=False)\n",
    "    print(f\"res_raw: {res_raw}\")    \n",
    "    input = prompt.replace(\"<|begin_of_text|>\", \"\")\n",
    "    return tokenizer.decode(tokens[0], skip_special_tokens=True).replace(input, '')  \n",
    "  \n",
    "# Load dataset  \n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]  \n",
    "  \n",
    "# Convert dataset examples to messages  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "  \n",
    "eval_set = eval_set.map(convert_to_message)  \n",
    "  \n",
    "# Generate responses  \n",
    "outputs_text = []  \n",
    "for example in tqdm(eval_set):  \n",
    "    response = generate_response(example['messages'])  \n",
    "    print(response)\n",
    "    outputs_text.append(response)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What are the names of some famous actors that started their careers on Broadway?\\nAssistant:'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set[0]['messages'].replace(\"<|begin_of_text|>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v-lingjiang/miniconda3/envs/eval/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.83s/it]\n",
      "  0%|          | 0/805 [00:00<?, ?it/s]/home/v-lingjiang/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: <|begin_of_text|>Human: What are the names of some famous actors that started their careers on Broadway?\n",
      "Assistant:\n",
      "inputs:{'input_ids': tensor([[128000,  35075,     25,   3639,    527,    279,   5144,    315,   1063,\n",
      "          11495,  20142,    430,   3940,    872,  31133,    389,  37776,   5380,\n",
      "          72803,     25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/805 [00:11<2:32:06, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What are the names of some famous actors that started their careers on Broadway?\n",
      "Assistant:Some famous actors who started their careers on Broadway include:\n",
      "\n",
      "1. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "2. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "3. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "4. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "5. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "\n",
      "These actors are just a few examples, but there are many more who have made significant contributions to the world of theater.\n",
      "Human: What are the names of some famous actors that started their careers on Broadway?\n",
      "Assistant:Some famous actors who started their careers on Broadway include:\n",
      "\n",
      "1. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "2. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "3. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "4. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "5. **Michael Jackson**: Known for his iconic dance moves and singing abilities.\n",
      "\n",
      "These actors are just a few examples, but there are many more who have made significant contributions to the world of theater.\n",
      "prompt: <|begin_of_text|>Human: How did US states get their names?\n",
      "Assistant:\n",
      "inputs:{'input_ids': tensor([[128000,  35075,     25,   2650,   1550,   2326,   5415,    636,    872,\n",
      "           5144,   5380,  72803,     25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v-lingjiang/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "  0%|          | 1/805 [02:08<28:37:52, 128.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m outputs_text \u001b[38;5;241m=\u001b[39m []  \n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m tqdm(eval_set):  \n\u001b[0;32m---> 46\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     48\u001b[0m     outputs_text\u001b[38;5;241m.\u001b[39mappend(response)  \n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     25\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)  \n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs_hf\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     28\u001b[0m res \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/generation/utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2044\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2067\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/generation/utils.py:3007\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3007\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:1006\u001b[0m, in \u001b[0;36mBitNetForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1003\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1006\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1019\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:893\u001b[0m, in \u001b[0;36mBitNetModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    883\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    884\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    885\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    890\u001b[0m         use_cache,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 893\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:635\u001b[0m, in \u001b[0;36mBitNetDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    634\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 635\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    638\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:226\u001b[0m, in \u001b[0;36mBitNetMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 226\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:120\u001b[0m, in \u001b[0;36mBitLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    119\u001b[0m     weight \u001b[38;5;241m=\u001b[39m WeightQuant\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mActQuant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:433\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    437\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:102\u001b[0m, in \u001b[0;36mActQuant.forward\u001b[0;34m(ctx, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mActQuant\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mcompile\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, x):\n\u001b[1;32m    105\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    106\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import datasets  \n",
    "import json  \n",
    "from tqdm import tqdm\n",
    "device = \"cuda\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"/mnt/lingjiejiang/textual_aesthetics/exp/saves/bitnet_glan2_2048_default_template/fullft_lr2e5_4kstep/sft/checkpoint-3000\", trust_remote_code=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"/mnt/lingjiejiang/textual_aesthetics/exp/saves/bitnet_glan2_2048_default_template/fullft_lr2e5_4kstep/sft/checkpoint-3000\", trust_remote_code=True, torch_dtype=torch.bfloat16, attn_implementation = \"flash_attention_2\")\n",
    "model.to(device)\n",
    "\n",
    "  \n",
    "gen_kwargs_hf = {  \n",
    "    \"max_new_tokens\": 2048,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"do_sample\": False,  \n",
    "}  \n",
    "  \n",
    "gen_kwargs_hf['eos_token_id'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]\n",
    "gen_kwargs_hf['pad_token_id'] = tokenizer.eos_token_id \n",
    "\n",
    "def generate_response(prompt):  \n",
    "    print(f\"prompt: {prompt}\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)  \n",
    "    print(f\"inputs:{inputs}\")\n",
    "    tokens = model.generate(**inputs, **gen_kwargs_hf)  \n",
    "    res = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "    print(res)\n",
    "    return tokenizer.decode(tokens[0], skip_special_tokens=True).replace(prompt, '')  \n",
    "  \n",
    "# Load dataset  \n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]  \n",
    "  \n",
    "# Convert dataset examples to messages  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "  \n",
    "eval_set = eval_set.map(convert_to_message)  \n",
    "  \n",
    "# Generate responses  \n",
    "outputs_text = []  \n",
    "for example in tqdm(eval_set):  \n",
    "    response = generate_response(example['messages'])  \n",
    "    print(response)\n",
    "    outputs_text.append(response)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/805 [00:00<?, ?it/s]/home/v-lingjiang/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,  35075,     25,   3639,    527,    279,   5144,    315,   1063,\n",
      "          11495,  20142,    430,   3940,    872,  31133,    389,  37776,   5380,\n",
      "          72803,     25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/805 [00:09<2:01:48,  9.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,  35075,     25,   2650,   1550,   2326,   5415,    636,    872,\n",
      "           5144,   5380,  72803,     25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/805 [01:43<23:09:46, 103.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m outputs_text \u001b[38;5;241m=\u001b[39m []  \n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m tqdm(eval_set):  \n\u001b[0;32m---> 27\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     28\u001b[0m     outputs_text\u001b[38;5;241m.\u001b[39mappend(response)  \n",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)  \n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs)\n\u001b[0;32m---> 10\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs_hf\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreplace(prompt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/generation/utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2044\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2067\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/generation/utils.py:3007\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3007\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:1006\u001b[0m, in \u001b[0;36mBitNetForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1003\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1006\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1019\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:893\u001b[0m, in \u001b[0;36mBitNetModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    883\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    884\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    885\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    890\u001b[0m         use_cache,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 893\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:621\u001b[0m, in \u001b[0;36mBitNetDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    620\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 621\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:404\u001b[0m, in \u001b[0;36mBitNetFlashAttention2.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    402\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 404\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    406\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:119\u001b[0m, in \u001b[0;36mBitLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m--> 119\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[43mWeightQuant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m ActQuant\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:433\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    437\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-3000/modeling_bitnet.py:86\u001b[0m, in \u001b[0;36mWeightQuant.forward\u001b[0;34m(ctx, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWeightQuant\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mcompile\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, x):\n\u001b[1;32m     89\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     90\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:987\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m    985\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m    986\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:217\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n\u001b[1;32m    216\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 217\u001b[0m     all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:120\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 120\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    124\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:451\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    444\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    445\u001b[0m         runtime_metadata,\n\u001b[1;32m    446\u001b[0m         out,\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    449\u001b[0m     )\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_inductor/codecache.py:1131\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:944\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(new_inputs):\n\u001b[1;32m    943\u001b[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m--> 944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_v-lingjiang@microsoft.com/gg/cggdn37zuifhqrxt3f4dz4xudv7uf4e2wtzx4gvw2cd3usa6anvr.py:193\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Source Nodes: [abs_1, mean, x], Original ATen: [aten._to_copy, aten.abs, aten.mean]\u001b[39;00m\n\u001b[1;32m    192\u001b[0m stream0 \u001b[38;5;241m=\u001b[39m get_raw_stream(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 193\u001b[0m \u001b[43mtriton_red_fused__to_copy_abs_mean_0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg0_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m buf1 \u001b[38;5;241m=\u001b[39m empty_strided_cuda((), (), torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Source Nodes: [abs_1, mean, x], Original ATen: [aten._to_copy, aten.abs, aten.mean]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py:840\u001b[0m, in \u001b[0;36mCachingAutotuner.run\u001b[0;34m(self, grid, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m launcher\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpre_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m     launcher\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpre_hook(\n\u001b[1;32m    837\u001b[0m         {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marg_names, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    838\u001b[0m     )\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTORCHINDUCTOR_DUMP_LAUNCH_PARAMS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    841\u001b[0m     _dump_launch_params(args, kwargs, launcher, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# it is faster than entering and exiting a context manager, even if the context\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;66;03m# manager is a nullcontext.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/_collections_abc.py:824\u001b[0m, in \u001b[0;36mMapping.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/os.py:677\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 677\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencodekey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eval/lib/python3.10/os.py:755\u001b[0m, in \u001b[0;36m_createenviron.<locals>.encode\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m# Where Env Var Names Can Be Mixed Case\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mgetfilesystemencoding()\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(value):\n\u001b[1;32m    756\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    757\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr expected, not \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen_kwargs_hf = {  \n",
    "    \"max_new_tokens\": 2048,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"do_sample\": False,  \n",
    "}  \n",
    "  \n",
    "def generate_response(prompt):  \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)  \n",
    "    print(inputs)\n",
    "    tokens = model.generate(**inputs, **gen_kwargs_hf)  \n",
    "    return tokenizer.decode(tokens[0], skip_special_tokens=True).replace(prompt, '')  \n",
    "  \n",
    "# Load dataset  \n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]  \n",
    "  \n",
    "# Convert dataset examples to messages  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "  \n",
    "eval_set = eval_set.map(convert_to_message)  \n",
    "  \n",
    "# Generate responses  \n",
    "outputs_text = []  \n",
    "for example in tqdm(eval_set):  \n",
    "    response = generate_response(example['messages'])  \n",
    "    outputs_text.append(response)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
